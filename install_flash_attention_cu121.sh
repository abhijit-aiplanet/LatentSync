#!/bin/bash

# Installs Flash Attention precompiled cu121 wheel for PyTorch 2.5.1 + Python 3.10
# Works on A100 GPUs in RunPod environment (driver CUDA 11.8 runtime is fine)

set -e

echo "üöÄ Installing Flash Attention (cu121) for PyTorch 2.5.1..."

source activate latentsync

echo "üßπ Removing conflicting packages..."
pip uninstall -y flash-attn xformers || true

# Step 1: Ensure correct PyTorch/TorchVision

echo "üì¶ Installing PyTorch 2.5.1 + cu121 ..."
pip install --force-reinstall torch==2.5.1 torchvision==0.20.1 --extra-index-url https://download.pytorch.org/whl/cu121

# Step 2: Install matching XFormers

echo "‚ö° Installing XFormers 0.0.28.post3 ..."
pip install xformers==0.0.28.post3 --extra-index-url https://download.pytorch.org/whl/cu121

# Step 3: Install precompiled Flash Attention wheel (cu121, torch2.5, py310)

echo "‚ö° Installing Flash Attention 2.8.0.post2 (precompiled cu12) ..."
# Wheel compiled for Python 3.10, Torch 2.5.x, CUDA 12 (works with cu121)
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2%2Bcu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

echo "üîç Verifying installation..."
python - << 'PY'
import torch, subprocess, os, sys
print(f"‚úÖ PyTorch: {torch.__version__}")
try:
    import flash_attn, xformers, triton
    from flash_attn.flash_attn_interface import flash_attn_func
    print(f"‚úÖ Flash Attention: {flash_attn.__version__}")
    print(f"‚úÖ XFormers: {xformers.__version__}")
    print(f"‚úÖ Triton: {triton.__version__}")
    print("üéâ Flash Attention kernel ready!")
except Exception as e:
    print("‚ùå Verification failed:", e)
    sys.exit(1)
PY

echo "üéâ Flash Attention cu121 installation SUCCESS!" 