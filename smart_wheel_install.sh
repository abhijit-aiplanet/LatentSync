#!/bin/bash

echo "ğŸ¯ SMART FLASH ATTENTION WHEEL INSTALLER"
echo "ğŸ” Finding compatible wheels automatically..."

# Activate environment
source $HOME/miniconda/bin/activate
source activate latentsync

# Clean previous attempts
pip uninstall -y flash-attn 2>/dev/null || true

# Get system info
PYTHON_VERSION="cp310"
PLATFORM="linux_x86_64"
TORCH_VERSION=$(python -c "import torch; print(torch.__version__.split('+')[0])")
CUDA_VERSION=$(python -c "import torch; print(torch.version.cuda)")

echo "ğŸ” System Info:"
echo "Python: $PYTHON_VERSION"
echo "PyTorch: $TORCH_VERSION"  
echo "CUDA: $CUDA_VERSION"
echo "Platform: $PLATFORM"

echo "ğŸ” Searching for compatible wheels..."

# List of potential wheel patterns to try
BASE_URL="https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3"

# Generate possible wheel names
WHEEL_PATTERNS=(
    "flash_attn-2.6.3+cu121torch2.5cxx11abiFALSE-${PYTHON_VERSION}-${PYTHON_VERSION}-${PLATFORM}.whl"
    "flash_attn-2.6.3+cu118torch2.5cxx11abiFALSE-${PYTHON_VERSION}-${PYTHON_VERSION}-${PLATFORM}.whl"
    "flash_attn-2.6.3+cu122torch2.5cxx11abiFALSE-${PYTHON_VERSION}-${PYTHON_VERSION}-${PLATFORM}.whl"
    "flash_attn-2.6.3+cu121torch2.4cxx11abiFALSE-${PYTHON_VERSION}-${PYTHON_VERSION}-${PLATFORM}.whl"
    "flash_attn-2.6.3-${PYTHON_VERSION}-${PYTHON_VERSION}-${PLATFORM}.whl"
)

# Function to test URL
test_url() {
    local url=$1
    echo "ğŸ”„ Testing: $(basename $url)"
    if curl --output /dev/null --silent --head --fail "$url"; then
        echo "âœ… Found: $url"
        return 0
    else
        echo "âŒ Not found: $(basename $url)"
        return 1
    fi
}

# Function to install and test wheel
install_and_test() {
    local url=$1
    echo "â¬‡ï¸ Installing: $(basename $url)"
    
    if pip install --force-reinstall --no-deps --no-cache-dir "$url"; then
        if python -c "import flash_attn; from flash_attn import flash_attn_func; print('âœ… Working!')" 2>/dev/null; then
            echo "ğŸ‰ SUCCESS: Flash Attention installed and working!"
            python -c "import flash_attn; print(f'Version: {flash_attn.__version__}')"
            return 0
        else
            echo "âŒ Installed but not working"
            return 1
        fi
    else
        echo "âŒ Installation failed"
        return 1
    fi
}

# Try each wheel pattern
for pattern in "${WHEEL_PATTERNS[@]}"; do
    wheel_url="$BASE_URL/$pattern"
    
    if test_url "$wheel_url"; then
        if install_and_test "$wheel_url"; then
            exit 0
        fi
    fi
done

echo "ğŸ” Trying alternative sources..."

# Try alternative repositories/sources
ALT_SOURCES=(
    "https://download.pytorch.org/whl/cu121/flash_attn-2.6.3%2Bcu121-cp310-cp310-linux_x86_64.whl"
    "https://files.pythonhosted.org/packages/flash_attn/flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl"
)

for source in "${ALT_SOURCES[@]}"; do
    if test_url "$source"; then
        if install_and_test "$source"; then
            exit 0
        fi
    fi
done

echo "ğŸ”§ Trying version-specific installations..."

# Try specific versions that are known to work better
for version in "2.5.8" "2.5.7" "2.5.6" "2.4.2"; do
    echo "ğŸ”„ Trying Flash Attention v$version..."
    
    if pip install --force-reinstall --no-cache-dir "flash-attn==$version" 2>/dev/null; then
        if python -c "import flash_attn; print(f'âœ… Version {version} works!')" 2>/dev/null; then
            echo "ğŸ‰ SUCCESS: Flash Attention v$version installed!"
            exit 0
        fi
    fi
done

echo "âŒ Smart wheel installation failed."
echo "ğŸ”§ Running comprehensive fix..."
exec ./comprehensive_flash_attention_fix.sh 