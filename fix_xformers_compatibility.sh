#!/bin/bash

echo "ğŸ”§ Fixing XFormers and Flash Attention Compatibility Issues..."
echo "============================================================"

# Make sure we're in the right environment
source activate latentsync

echo "ğŸ’¥ Step 1: Removing incompatible packages..."
pip uninstall xformers flash-attn triton -y 2>/dev/null || true

echo "âœ… Cleanup complete!"

echo "ğŸ”§ Step 2: Installing compatible XFormers for PyTorch 2.5.1..."
pip install xformers==0.0.28.post3 --extra-index-url https://download.pytorch.org/whl/cu121

echo "âœ… XFormers installed!"

echo "ğŸš€ Step 3: Installing compatible Flash Attention..."
pip install packaging ninja
pip install flash-attn==2.5.9.post1 --no-build-isolation

echo "âœ… Flash Attention installed!"

echo "âš¡ Step 4: Installing Triton for GPU acceleration..."
pip install triton==2.3.1

echo "âœ… Triton installed!"

echo "ğŸ” Step 5: Verifying installations..."
python -c "
import torch
print(f'âœ… PyTorch: {torch.__version__}')

try:
    import xformers
    print(f'âœ… XFormers: {xformers.__version__}')
    # Test XFormers functionality
    import xformers.ops
    print('âœ… XFormers ops: Available')
except Exception as e:
    print(f'âŒ XFormers error: {e}')

try:
    import flash_attn
    print(f'âœ… Flash Attention: Available')
    from flash_attn.flash_attn_interface import flash_attn_func
    print('âœ… Flash Attention func: Available')
except Exception as e:
    print(f'âŒ Flash Attention error: {e}')

try:
    import triton
    print(f'âœ… Triton: {triton.__version__}')
except Exception as e:
    print(f'âŒ Triton error: {e}')
"

echo ""
echo "ğŸ‰ COMPATIBILITY FIX COMPLETE!"
echo "=============================="
echo "âœ… XFormers reinstalled for PyTorch 2.5.1"
echo "âœ… Flash Attention reinstalled for Python 3.10"
echo "âœ… Triton installed for GPU acceleration"
echo ""
echo "ğŸš€ Now run: ./start_gradio_xformers.sh" 